{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":120126,"databundleVersionId":14369730,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q librosa xgboost tqdm\n!pip install -q git+https://github.com/openai/whisper.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T17:13:40.461936Z","iopub.execute_input":"2025-12-15T17:13:40.462232Z","iopub.status.idle":"2025-12-15T17:15:34.654897Z","shell.execute_reply.started":"2025-12-15T17:13:40.462210Z","shell.execute_reply":"2025-12-15T17:15:34.653132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import whisper\nprint(whisper.available_models())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T17:16:27.363912Z","iopub.execute_input":"2025-12-15T17:16:27.364278Z","iopub.status.idle":"2025-12-15T17:16:35.329648Z","shell.execute_reply.started":"2025-12-15T17:16:27.364231Z","shell.execute_reply":"2025-12-15T17:16:35.328643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport librosa\nimport whisper\nimport re\n\nfrom tqdm import tqdm\nfrom xgboost import XGBRegressor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T17:17:29.449319Z","iopub.execute_input":"2025-12-15T17:17:29.450082Z","iopub.status.idle":"2025-12-15T17:17:32.401827Z","shell.execute_reply.started":"2025-12-15T17:17:29.450042Z","shell.execute_reply":"2025-12-15T17:17:32.400774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(\n    \"/kaggle/input/shl-intern-hiring-assessment-2025/dataset/csvs/train.csv\"\n)\n\ntest_df = pd.read_csv(\n    \"/kaggle/input/shl-intern-hiring-assessment-2025/dataset/csvs/test.csv\"\n)\n\nprint(train_df.head())\nprint(train_df.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T17:18:12.661091Z","iopub.execute_input":"2025-12-15T17:18:12.661456Z","iopub.status.idle":"2025-12-15T17:18:12.678247Z","shell.execute_reply.started":"2025-12-15T17:18:12.661431Z","shell.execute_reply":"2025-12-15T17:18:12.677326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_text_features(text):\n    text = text.lower().strip()\n    words = text.split()\n\n    num_words = len(words)\n    avg_word_len = sum(len(w) for w in words) / num_words if num_words > 0 else 0\n    num_sentences = len(re.findall(r'[.!?]', text)) + 1 if text else 0\n    lexical_diversity = len(set(words)) / num_words if num_words > 0 else 0\n\n    return np.array([\n        num_words,\n        avg_word_len,\n        num_sentences,\n        lexical_diversity\n    ])\n\n\ndef speech_rate(text, duration):\n    words = text.split()\n    return len(words) / duration if duration > 0 else 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T17:18:14.816412Z","iopub.execute_input":"2025-12-15T17:18:14.817341Z","iopub.status.idle":"2025-12-15T17:18:14.826088Z","shell.execute_reply.started":"2025-12-15T17:18:14.817302Z","shell.execute_reply":"2025-12-15T17:18:14.825159Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_audio_features(audio_path):\n    try:\n        y, sr = librosa.load(audio_path, sr=16000)\n        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n\n        feats = []\n        for stat in [np.mean, np.std, np.min, np.max]:\n            feats.extend(stat(mfcc, axis=1))\n\n        duration = librosa.get_duration(y=y, sr=sr)\n\n        return np.array(feats), duration   # 52 MFCC features + duration\n    except:\n        return np.zeros(52), 0.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T17:21:30.773072Z","iopub.execute_input":"2025-12-15T17:21:30.773549Z","iopub.status.idle":"2025-12-15T17:21:30.780873Z","shell.execute_reply.started":"2025-12-15T17:21:30.773520Z","shell.execute_reply":"2025-12-15T17:21:30.779891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"asr_model = whisper.load_model(\"base\")\n\ndef transcribe_audio(audio_path):\n    try:\n        result = asr_model.transcribe(audio_path)\n        return result[\"text\"]\n    except:\n        return \"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T17:21:48.741769Z","iopub.execute_input":"2025-12-15T17:21:48.742359Z","iopub.status.idle":"2025-12-15T17:21:51.019335Z","shell.execute_reply.started":"2025-12-15T17:21:48.742332Z","shell.execute_reply":"2025-12-15T17:21:51.018423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = []\ny = []\n\nfor _, row in tqdm(train_df.iterrows(), total=len(train_df)):\n    audio_path = (\n        \"/kaggle/input/shl-intern-hiring-assessment-2025/\"\n        f\"dataset/audios/train/{row['filename']}.wav\"\n    )\n\n    text = transcribe_audio(audio_path)\n    audio_feat, duration = extract_audio_features(audio_path)\n    text_feat = extract_text_features(text)\n    rate = speech_rate(text, duration)\n\n    features = np.concatenate([\n    audio_feat,        # 52 MFCC stats\n    text_feat,         # 4 text features\n    np.array([rate])   # 1 speech-rate feature\n    ])\n\nX.append(features)\ny.append(row[\"label\"])\n\nX = np.array(X)\ny = np.array(y)\n\nprint(\"X shape:\", X.shape)\nprint(\"y shape:\", y.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T17:22:45.810441Z","iopub.execute_input":"2025-12-15T17:22:45.811483Z","iopub.status.idle":"2025-12-15T18:17:52.490834Z","shell.execute_reply.started":"2025-12-15T17:22:45.811449Z","shell.execute_reply":"2025-12-15T18:17:52.488407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = XGBRegressor(\n    n_estimators=300,\n    learning_rate=0.05,\n    max_depth=5,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    random_state=42\n)\n\nmodel.fit(X, y)\nprint(\"Model training complete\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T18:19:47.458070Z","iopub.execute_input":"2025-12-15T18:19:47.460917Z","iopub.status.idle":"2025-12-15T18:19:47.638985Z","shell.execute_reply.started":"2025-12-15T18:19:47.460861Z","shell.execute_reply":"2025-12-15T18:19:47.637959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_preds = []\n\nfor _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n    audio_path = (\n        \"/kaggle/input/shl-intern-hiring-assessment-2025/\"\n        f\"dataset/audios/test/{row['filename']}.wav\"\n    )\n\n    text = transcribe_audio(audio_path)\n    audio_feat, duration = extract_audio_features(audio_path)\n    text_feat = extract_text_features(text)\n    rate = speech_rate(text, duration)\n\n    features = np.concatenate([\n        audio_feat,\n        text_feat,\n        np.array([rate])\n    ])\n\n    pred = model.predict(features.reshape(1, -1))[0]\n    test_preds.append(pred)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T18:48:45.440313Z","iopub.execute_input":"2025-12-15T18:48:45.440691Z","iopub.status.idle":"2025-12-15T19:11:17.224500Z","shell.execute_reply.started":"2025-12-15T18:48:45.440666Z","shell.execute_reply":"2025-12-15T19:11:17.221837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame({\n    \"filename\": test_df[\"filename\"],\n    \"label\": test_preds\n})\n\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T19:28:41.319866Z","iopub.execute_input":"2025-12-15T19:28:41.320215Z","iopub.status.idle":"2025-12-15T19:28:41.334335Z","shell.execute_reply.started":"2025-12-15T19:28:41.320189Z","shell.execute_reply":"2025-12-15T19:28:41.333437Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"    filename  label\n0  audio_141    4.5\n1  audio_114    4.5\n2   audio_17    4.5\n3   audio_76    4.5\n4  audio_156    4.5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>audio_141</td>\n      <td>4.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>audio_114</td>\n      <td>4.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>audio_17</td>\n      <td>4.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>audio_76</td>\n      <td>4.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>audio_156</td>\n      <td>4.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(\"submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T19:20:29.070023Z","iopub.execute_input":"2025-12-15T19:20:29.070357Z","iopub.status.idle":"2025-12-15T19:20:29.077602Z","shell.execute_reply.started":"2025-12-15T19:20:29.070333Z","shell.execute_reply":"2025-12-15T19:20:29.076614Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/submission.csv","text/html":"<a href='submission.csv' target='_blank'>submission.csv</a><br>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}